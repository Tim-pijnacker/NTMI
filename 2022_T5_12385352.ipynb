{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHjuHXpIv4QI"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probabll/ntmi-tutorials/blob/main/T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvWfS-ELNzE"
      },
      "source": [
        "# Guide\n",
        "\n",
        "* Check the entire notebook before you get started, this gives you an idea of what lies ahead.\n",
        "* Note that, as always, the notebook contains a condensed version of the theory We recommend you read the theory part before the LC session.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqR7WUDXLeME"
      },
      "source": [
        "## ILOs\n",
        "\n",
        "After completing this lab you should be able to\n",
        "\n",
        "* develop NGram LMs (classic and neural) in Python (and PyTorch)\n",
        "* estimate parameters of LMs via MLE\n",
        "* evaluate LMs intrinsically in terms of perplexity\n",
        "* evaluate LMs statistically in terms of properties of generated text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBR2bPwLL9gj"
      },
      "source": [
        "## General Notes\n",
        "\n",
        "* In this notebook you are expected to use $\\LaTeX$. \n",
        "* Use python3.\n",
        "* Use Torch\n",
        "* To have GPU support run this notebook on Google Colab (you will find more instructions later).\n",
        "\n",
        "We will use a set of standard libraries that are often used in machine learning projects. If you are running this notebook on Google Colab, all libraries should be pre-installed. If you are running this notebook locally you will need to install some additional packages, ask your TA for help if you have problems setting up.\n",
        "\n",
        "If you need a short introduction to PyTorch [check this tutorial](https://github.com/probabll/ntmi-tutorials/blob/main/PyTorch.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqDZh0QJJsOu"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "* [Data](#data)\n",
        "* [Word segmentation and tokenisation](#bpe)\n",
        "* [Language models](#lms)\n",
        "* [Evaluation](#evaluation)\n",
        "* [Neural NGram LM](#neural)\n",
        "    * [Experiment](#neural-experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg54PiqdLoN4"
      },
      "source": [
        "## Table of Graded Exercises\n",
        "\n",
        "**Important.** The grader may re-run your notebook to investigate its correctness, but you do upload your notebook with the cells already run and make sure that all your answers are visible without the need to re-run the notebook. \n",
        "\n",
        "The weight of the exercise is indicated below.\n",
        "\n",
        "* [Neural NGram LMs](#report) (70%)\n",
        "* [Analysis](#stats) (30%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVnfg0kMLrsi"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzjCfsJDNL1B"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "    \n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV4oRuW-XYED"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install sentencepiece\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8qybX6RNDKh"
      },
      "source": [
        "# <a name=\"data\"/> Data\n",
        "\n",
        "In this tutorial we will develop models of text generation. So our data for this tutorial will be collections of sentences, or *corpora*. We will use corpora available in NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfbZPjfaZDdo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNqS36sZNExH"
      },
      "outputs": [],
      "source": [
        "def split_nltk_corpus(corpus, max_length=30, rng=np.random.RandomState(42)):\n",
        "    \"\"\"\n",
        "    Shuffle and split a corpus.\n",
        "    corpus: list of sentences, each sentence is a list of tokens, each token is a string.\n",
        "    max_length: discard sentences longer than this\n",
        "\n",
        "    Return: training sentences, dev sentences, test sentences\n",
        "        in each corpus a sentence is now a string where tokens are space separated\n",
        "    \"\"\"\n",
        "    sentences = corpus.sents()\n",
        "    # do not change the seed in here    \n",
        "    order = rng.permutation(np.arange(len(sentences)))    \n",
        "    shuffled = [' '.join(sentences[i]) for i in order if len(sentences[i]) <= max_length]    \n",
        "    return shuffled[2000:], shuffled[1000:2000], shuffled[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-pkD_jNOHBj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown, treebank\n",
        "\n",
        "# if you are developing/debugging on CPU, use `treebank`\n",
        "# for the experiments in the assignment, you should use `brown`\n",
        "# that's because `treebank` is too small for a neural network\n",
        "\n",
        "training, dev, test = split_nltk_corpus(brown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74md717DKLc9"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of sentences: training={len(training)} dev={len(dev)} test={len(test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk5nks5bKG0a"
      },
      "outputs": [],
      "source": [
        "print(\"# A few training sentences\\n\\n\")\n",
        "for x in training[:10]:\n",
        "    print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6hEpj0DOzxO"
      },
      "source": [
        "# <a name=\"bpe\"/> Word segmentation and tokenisation\n",
        "\n",
        "Our models of text generation are probability distributions over finite-length sequences of discrete symbols. These discrete symbols are what we call *tokens*. The **vocabulary** of the model is therefore the finite set of known tokens which it can use to make sequences. \n",
        "\n",
        "We are interested in a special type of sequence, namely, sentences. Sentences are typically made of linguistic units that we call words. The linguistic notion of *word* is much too complex for our models. In practice, we use a data-driven and computationally convenient notion instead.\n",
        "\n",
        "In this tutorial we will work with tokens that are subword units obtained via a compression algorithm known as *byte pair encoding* (BPE). You can optionally check the [original paper](http://www.aclweb.org/anthology/P16-1162) for more detail. \n",
        "\n",
        "We will use a package called `sentencepiece` that implements an efficient BPE tokeniser (and word segmenter) for us. This tokeniser is language independent, it learns a vocabulary of subword units from a corpus of sentences (without the need for any tokenisation). Because this is based on a compression algorithm, we can choose the level of compression, that is, we can choose the number of tokens that we want to have in the vocabulary, the BPE algorithm will find what collection of tokens best describes the corpus at the given budget. \n",
        "\n",
        "After trained, we can use the BPE model to tokenise and detokenise sentences for us deterministically. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q9Vx37hT_1l"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pEGOnBb5xRl"
      },
      "source": [
        "This helper function trains a BPE model for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cMWg7AnUE1E"
      },
      "outputs": [],
      "source": [
        "def fit_vocabulary(corpus, vocab_size):\n",
        "    \"\"\"\n",
        "    Return a BPE model as implemented by the sentencepiece package.\n",
        "\n",
        "    corpus: an iterable of sentences, each sentence is a python string\n",
        "    \"\"\"\n",
        "    proto = io.BytesIO()\n",
        "\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        sentence_iterator=iter(corpus), \n",
        "        model_writer=proto, \n",
        "        vocab_size=vocab_size,\n",
        "        pad_id=0,\n",
        "        bos_id=1,\n",
        "        eos_id=2,\n",
        "        unk_id=3,\n",
        "    )\n",
        "\n",
        "    return spm.SentencePieceProcessor(model_proto=proto.getvalue())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--9I-m4sXO6M"
      },
      "outputs": [],
      "source": [
        "tokenizer = fit_vocabulary(training, vocab_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE0kDLoU7sw_"
      },
      "source": [
        "Note how we control the vocabulary size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwDYZnQb7uzK"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQwWTQhQ7v7C"
      },
      "source": [
        "And see how we can use this object to tokenize and detokenize text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTGnu6Ce6X_6"
      },
      "outputs": [],
      "source": [
        "example_str = training[1]\n",
        "print(example_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLsaq2ht6bMp"
      },
      "source": [
        "The function `encode` can be used to tokenize a string into a list of tokens (each a string). To be able to read the output, you need to use the argument `out_type=str`, otherwise the tokenizer will convert the tokens to numerical codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Ma5982633i"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(example_str, out_type=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gMpdG7N6wAp"
      },
      "source": [
        "The `decode` method can map the tokens back to original form:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrR9lypI6wJ3"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(example_str, out_type=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mmZQwUk6pYL"
      },
      "source": [
        "Without `out_type=str` we get a sequence of codes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlDeuNoIV43l"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(example_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coFQMcyY6t-_"
      },
      "source": [
        "And, of course, `decode` can map it back to text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po5bVZndWLZ8"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode(example_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOgw51RB7Lup"
      },
      "source": [
        "There are two main advantages of this strategy for tokenization:\n",
        "\n",
        "1. we control the vocabulary size (which helps us control memory usage)\n",
        "2. oftentimes unseen words are made of a combination of existing subword units, so we can deal with more words than before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LitMBNpm7awk"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"This is a tutorial within Natuurlijke Talmodellen en Interfaces at the UvA.\", out_type=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po_HqEgsmTFl"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"This is a tutorial within Natuurlijke Talmodellen en Interfaces at the UvA.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBZfphivEmDE"
      },
      "source": [
        "You can also preprocess whole batches of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw2g0klsEo5v"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode([\"This is a sentence.\",  \"And this is another\"], out_type=str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCVRuWpfEvKZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenizer.encode([\"This is a sentence.\",  \"And this is another\"], out_type=str))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmFEQrPlMIFp"
      },
      "source": [
        "**Ungraded exercise.** Play a bit with the tokenizer object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVKUr3UcybC6"
      },
      "source": [
        "# <a name=\"lms\"/> Language Models\n",
        "    \n",
        "A language model (LM) is a **probability distribution over text**, where text is a finite sequences of words. \n",
        "    \n",
        "LMs can be used to generate text as well as to quantify a degree of naturalness (or rather, a degree of resemblance to training data) which is useful for example to compare and rank alternative pieces of text in terms of fluency.\n",
        "    \n",
        "To design an LM, we need to talk about units of text (e.g., documents, paragraphs, sentences) as outcomes of a random experiment, and for that we need random variables.\n",
        "    \n",
        "We will generally refer to the unit of text as a *sentence*, but that's just for convenience, you could design an LM over documents and very little, if anything, would change.  \n",
        "\n",
        "A **random sentence** is a finite sequence of symbols from the vocabulary of a given language. As a running example, we will refer to this language as English. The vocabulary of English will be denoted by $\\mathcal W$, a finite collection of unique symbols, each of which we refer to as a *word* (but in practice, these symbols are any unit we care to model). We will denote a random sentence by $X$, or more explicitly, by the random sequence $X = \\langle W_1, \\ldots, W_L \\rangle$. Here $L$ indicates the sequence length. Each word in the sequence is a random variable that takes on values in $\\mathcal W$. We will adopt an important convention, every sentence is a finite sequence that ends with a special symbol, the end-of-sequence (EOS) symbol. \n",
        "\n",
        "Formally, random sentences take on values in the set $\\mathcal W^*$ of all strings made of symbols in $\\mathcal W$, which is a set that does include an infinte number of valid English sentences (possibly not all English sentences, as our vocabulary may not be complete enough, but hopefully this space is still large enough for the LM to be useful) as well as an infinite number of sequences that are not valid English sentences.       \n",
        "    \n",
        "Part of the deal with a language model is to define and estimate a probability distribution that expresses a preference for sentences that are more likely to be accepted as English sentences. In practice an LM will prefer sentences that reproduce statistics of the observations used to estimate its parameters, whether these sentences will resemble English sentences or not will depend on how expressive the LM is, that is, whether or not the LM can capture patterns as complex as those arising from well-formed English (or whatever variant/register of English was observed during training).\n",
        "    \n",
        "**Notation guide** Some textbooks or papers use $W_1^L$ instead of $W_{1:L}$ for ordered sequences, both are clear enough, but we will use the notation adopted by the textbook, that is, $W_{1:L}$. The textbook uses $W_1\\cdots W_L$ (without commas) as another notation for ordered sequences, but we prefer to explicitly mark the sequence with angle brackets to avoid ambiguities, i.e., we prefer $\\langle W_1, \\ldots, W_L \\rangle$. For assignments, we will use the lowercase version of the letter that names the random variable: $w_{1:l} = \\langle w_1, \\ldots, w_l \\rangle$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyxN3qpz-nw"
      },
      "source": [
        "The following notational shortcuts are rather convenient:\n",
        "\n",
        "* we will often use $W_{1:L}$ for a random sentence instead of the longer form $\\langle W_1, \\ldots, W_L \\rangle$, and similarly for outcomes (i.e., $w_{1:l}$ instead of $\\langle w_1, \\ldots, w_l\\rangle$), but in the long form we shall never drop the angle brackets, as otherwise it's hard to tell that we mean an ordered sequence\n",
        "* we will use $W_{<i}$ (or $w_{<i}$ for an outcome) to denote the sequence of tokens that precedes the $i$th token, this sequence is empty $W_{<i} \\triangleq \\langle \\rangle$ for $i \\le 1$, for $i>1$ the sequence is defined as $W_{<i} \\triangleq \\langle W_1, \\ldots, W_{i-1}\\rangle$\n",
        "* sometimes it will be useful to find a more compact notation for $W_{<i}$, in those cases we refer to it as a *random history* and denote it by $H$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xrSiKo0bcl"
      },
      "source": [
        "LMs can be described by a **generative story**, that is, a stochastic procedure that explains how an outcome $w_{1:l}$ is drawn from the model distribution. Though we may find inspiration in how we believe the data were generated, the generative story is not a faithful representation of any linguistic process, it is all but an abstraction that codes our own assumptions about the problem. \n",
        "\n",
        "The most general form this generative story can take, that is, the form with the least amount of assumptions, looks as follows:\n",
        "\n",
        "1. For each position $i$ of a sequence, condition on the history $h_i$ and draw the $i$th word $w_i$ with probability $P(W=w_i|H=h_i)$. \n",
        "2. Append $w_i$ to the end of the history: $h_i \\circ \\langle w_i \\rangle$\n",
        "2. Stop generating if $w_i$ is the EOS token, else repeat from (1).\n",
        "\n",
        "We say this procedure is very general because it is essentially just chain rule spelled out in English words, though here the order of enumeration is determined by the left-to-right order of tokens in an English sentence.\n",
        "\n",
        "\n",
        "Here is an example for a sequence of length $l=3$:\n",
        "\n",
        "$P_X(\\langle w_1, w_2, w_3 \\rangle) = P_{W|H}(w_1|\\langle \\rangle) P_{W|H}(w_2|\\langle w_1 \\rangle) P_{W|H}(w_3 |\\langle w_1, w_2 \\rangle)$\n",
        "\n",
        "For our example sentence *He went to the store* this means:\n",
        "\n",
        "\\begin{align}\n",
        "P_X(\\langle \\text{He, went, to, the, store, EOS} \\rangle) &= P_{W|H}(\\text{He}|\\langle \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{went}|\\langle \\text{He} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{to}|\\langle \\text{He}, \\text{went} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{the}|\\langle \\text{He},  \\text{went}, \\text{to} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{store}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the} \\rangle) \\\\\n",
        "    &\\times P_{W|H}(\\text{EOS}|\\langle \\text{He},  \\text{went}, \\text{to}, \\text{the}, \\text{store} \\rangle) \n",
        "\\end{align}\n",
        "\n",
        "* where with some abuse of notation we use the words themselves as outcomes instead of their corresponding indices. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QseSMY5K1QP1"
      },
      "source": [
        "**Exercise with solution**  Write down the general rule for the probability $P_X$ of a sentence $w_{1:l}$. Don't forget to indicate the precise random variable associated with every distribution (that is, for example, $P_X(w_{1:l})$ and $P(X=w_{1:l})$ are correct while $P(w_{1:l})$ is incomplete). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMkvbZrS1Sd9"
      },
      "source": [
        "<details>\n",
        "    <summary><b>SOLUTION</b></summary>\n",
        "\n",
        "$P_X(w_{1:l}) = \\prod_{i=1}^{l}P_{W|H}(w_i|w_{<i})$\n",
        "\n",
        "where $l$ is the length of the sentence.\n",
        "    \n",
        "</details>\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EysRkGqW1vDp"
      },
      "source": [
        "The LM above is just an abstraction, not a concrete implementation, think of it as a general template for building models. \n",
        "\n",
        "A concrete model design needs to specify the conditional probability distributions in the model, this is known as the **parameterisation** of the model, and an algorithm for parameter estimation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYcaZEYgJzJz"
      },
      "source": [
        "# <a name=\"evaluation\"/> Evaluation\n",
        "\n",
        "There are a few ways to evaluate the performance of an LM.\n",
        "\n",
        "Sometimes you can plug it into an application (e.g., an auto-complete system or a system that ranks sentence for fluency), in those cases we can test whether that downstream application improves as we modify the language model. This is called *extrinsic* evaluation.\n",
        "\n",
        "To evaluate an LM independently from an application we need to evaluate its statistical properties in an attempt to determine how well the model fits the data, namely, how well it reproduces statistics of observed data. This is called *intrinsinc* evaluation. In this course, we are going to focus on intrinsic evaluation of the LM.\n",
        "\n",
        "We generally have access to 3 datasets: \n",
        "\n",
        "* Training is used for estimating $\\theta$.\n",
        "* Develpment is used to make choices during the design phase (choose hyperparameters such as smoothing technique, order of model, etc).\n",
        "* Test is used for measuring the accuracy of the final model.\n",
        "   \n",
        "One indication of the model's fitness to the data is the value of the model likelihood given novel sentences (e.g., sentence held-out from training). \n",
        "We assume this dataset $\\mathcal T$ of novel sentences consits of $K$ independent sentences each denoted $w_{1:l_k}^{(k)}$, then the model likelihood given $\\mathcal T$ is the probability mass that the model assigns to $\\mathcal T$: \n",
        "\n",
        "$\\prod_{k=1}^K f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "or in form of the log-probability:\n",
        "\n",
        "$\\sum_{k=1}^K \\log f_{X}(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "Then define the log-likelihood as follows:\n",
        "\n",
        "$\\mathcal L_{\\mathcal T}(\\theta) = \\sum_{k=1}^K \\log f_X(w_{1:l_k}^{(k)}; \\theta)$\n",
        "\n",
        "\n",
        "Then the model that assings the higher $\\mathcal L$ given the test set is the one that's better predictive of future data, presumably that's the case because it found a better fit of the training data (a better compromise between memorisation and generalisation). \n",
        "\n",
        "In other words, given two probabilistic models, the one that assigns a higher probability to the test data is taken as intrinsically better. One detail we need to abstract away from is differences in factorisation of the models which may cause their likelihoods not to be comparable, but for that we will define *perplexity* below. \n",
        "\n",
        "The log likelihood is used because the probability of a particular sentence according to the LM can be a very small number, and the product of these small numbers can become even smaller, and it will cause numerical\n",
        "precision problems. \n",
        "\n",
        "\n",
        "**Perplexity** of a language model on a test set is the inverse probability of the test set, normalized\n",
        "by the number of tokens. Perplexity is a notion of average branching factor, thus an LM with low perplexity can be thought of as a *less confused* LM. That is, each time it introduces a word given some history it picks from a reduced subset of the entire vocabulary (in other words, it is more certain of how to continue from the history). \n",
        "\n",
        "If a dataset contains $t$ tokens where $t = \\sum_{k=1}^K l_k$, then the perplexity of the model given the test set is\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{PP}_{\\mathcal T}(\\theta) = \\exp\\left( -\\frac{1}{t} \\mathcal L_{\\mathcal T}(\\theta) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "The lower the perplexity, the better the model is. Comparisons in terms of perplexity are only fair if the models have the same vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlqKGKLn1FSH"
      },
      "source": [
        "# <a name=\"neural\"/> Neural NGram LM\n",
        "\n",
        "Just like the classic $n$-gram LM, a neural $n$-gram LM makes a conditional independence assumption to simplify the factors in the chain rule. Rather than storying the probabilities values of observed $(h, w)$ pairs, the neural model stores the parameters necessary to predict those probabilities by transformation of the concatenation of the one-hot encodings of the words in the history. The pmf of the neural $n$-gram LM is defined as follows:\n",
        "\n",
        "\\begin{align}\n",
        "f_X(w_{1:l};\\theta) &= \\prod_{i=1}^l \\mathrm{Cat}(w_i|\\mathbf g(w_{i-n+1:i-1}; \\theta)) \n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbf g$ is a neural network with parameters $\\theta$, it maps a tuple of words $w_{i-n+1:i-1}$, from a vocabulary of $V$ known words, to a $V$-dimensional probability vector.\n",
        "\n",
        "This neural network is typically implemented as follows:\n",
        "\n",
        "* embed each word in the history into a $D$-dimensional space: $\\mathbf e_j = \\mathrm{embed}_D(w_j; \\theta_{\\text{in}})$\n",
        "* concatenate the word embeddings for the words in the history: $\\mathbf u_i = \\mathrm{concat}(\\mathbf e_{i-n+1}, \\ldots, \\mathbf e_{i-1})$\n",
        "* use a single-layer feed-forward NN to transform the history encoding $\\mathbf u_i$ into a vector of $V$ logits: $\\mathbf s = \\mathrm{ffnn}_V(\\mathbf u_i; \\theta_{\\text{out}})$\n",
        "* use softmax to obtain probabilities for the possible words at the $i$th position: $\\mathbf g(w_{i-n+1:i-1}; \\theta)$\n",
        "* the parameters are $\\theta = \\theta_{\\text{in}} \\cup \\theta_{\\text{out}}$ where \n",
        "    * $\\theta_{\\text{in}}$ is an embedding matrix $\\mathbf E \\in \\mathbb R^{V\\times D}$\n",
        "    * $\\theta_{\\text{out}}$ are the parameters of the hidden layer $\\mathbf W^{[1]} \\in \\mathbb R^{H\\times (n-1)D}$ and $\\mathbf b^{[1]} \\in \\mathbb R^H$, and the parameters of the output layer $\\mathbf W^{[2]} \\in \\mathbb R^{V\\times H}$ and $\\mathbf b^{[2]} \\in \\mathbb R^V$\n",
        "\n",
        "\n",
        "Next we implement this in PyTorch using `torch.nn.Embedding` for the embedding layer and `torch.nn.Linear` for the affine transformations inside the FFNN. As non-linearity for hidden layers we will use ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_z8N_ZpTNMy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_all(seed=42):    \n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)    \n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "seed_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf3iDZ-nhL9E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as td\n",
        "import torch.optim as opt\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0KNq86diKB"
      },
      "source": [
        "**Ungraded exercise** Study the NeuralNGram LM class below and complete the code for the forward pass and for the loss function. We have a test case that you can use to check your implementation. Once you are satisfied, compare your solution to ours, if they differ understand what you did wrong and use ours for the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPa1nCVBhP4X"
      },
      "outputs": [],
      "source": [
        "class NeuralNGramLM(nn.Module):\n",
        "\n",
        "    def __init__(self, ngram_size: int, vocab_size, embedding_dim: int, hidden_size: int, pad_id=0, bos_id=1, eos_id=2):\n",
        "        \"\"\"\n",
        "        ngram_size: longest ngram\n",
        "        vocab_size: number of known words\n",
        "        embedding_dim: dimensionality of word embeddings\n",
        "        hidden_size: dimensionality of hidden layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert ngram_size > 1, \"This class expects at least ngram_size 2\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ngram_size = ngram_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pad = pad_id\n",
        "        self.bos = bos_id\n",
        "        self.eos = eos_id\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
        "        self.logits_predictor = nn.Sequential(\n",
        "            nn.Linear((ngram_size - 1) * embedding_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, vocab_size),\n",
        "        )\n",
        "\n",
        "    def num_parameters(self):\n",
        "        return sum(np.prod(theta.shape) for theta in self.parameters())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
        "\n",
        "        This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
        "        It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "\n",
        "        Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "        \"\"\"\n",
        "        # The inputs to the FFNN are the ngram_size-1 previous words:\n",
        "\n",
        "        # Create a sequence of BOS symbols to be prepended to x.\n",
        "        # [batch_size, ngram_size - 1]\n",
        "        bos = torch.full((x.shape[0], self.ngram_size - 1), self.bos, device=x.device)\n",
        "        # [batch_size, max_length + ngram_size - 1]\n",
        "        _x = torch.cat([bos, x], 1)\n",
        "\n",
        "        # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
        "        # [batch_size, max_length, ngram_size - 1]\n",
        "        inputs = torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x.shape[0], 1, -1) for i in range(x.shape[1])], 1)\n",
        "        \n",
        "        raise NotImplementedError(\"Use the layers in this model to predict a batch `s` of logits with shape [batch_size, max_length, vocab_size] for a batch of `inputs` with shape [batch_size, max_length, ngram_size - 1]\")\n",
        "        s = None  # use your own implementation (or copy ours from the answer model below)\n",
        "\n",
        "        # For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
        "        # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
        "        return td.Categorical(logits=s)\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        \"\"\"\n",
        "        Computes the log probability of each sentence in a batch.\n",
        "\n",
        "        x: [batch_size, max_length]\n",
        "        \"\"\"\n",
        "        # [batch_size, max_length]\n",
        "        logp = self(x).log_prob(x)\n",
        "        # [batch_size]\n",
        "        logp = torch.where(x != self.pad, logp, torch.zeros_like(logp)).sum(-1)\n",
        "        return logp    \n",
        "\n",
        "    def sample(self, batch_size=1, max_length=50):\n",
        "        \"\"\"\n",
        "        Draws a number of samples from the model, each sample is a complete sequence.\n",
        "        We impose a maximum number of steps, to avoid infinite loops.\n",
        "        This procedure takes care of mapping sampled symbols to pad after the EOS symbol is generated.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():  # sampling discrete outcomes is not differentiable\n",
        "            # Reserve memory for the samples (it's not important what symbol to use, I use BOS for clarity)\n",
        "            x = torch.full((batch_size, max_length), self.bos, device=self.embed.weight.device) \n",
        "            # Keeps track of which samples are complete (i.e., already include EOS)\n",
        "            complete = torch.zeros(batch_size, device=self.embed.weight.device)\n",
        "\n",
        "            for i in range(max_length):\n",
        "                # We condition on x[:,:i] (the prefixes) and parameterise Categoricals per step\n",
        "                #  then sample tokens. This will sample all tokens (including tokens in the prefix), \n",
        "                #  but we are only interested in the 'current' one, which we use to udpate our\n",
        "                #  actual sample x\n",
        "                # [batch_size, length]\n",
        "                x_i = self(x[:,:i+1]).sample()\n",
        "                # Here we update the current token to something freshly sampled\n",
        "                #  and also replace the token by 0 (pad) in case the sentence is already complete\n",
        "                x[:, i] = x_i[:, i] * (1 - complete)\n",
        "                # Here we update the state of the sentence (i.e., complete or not).\n",
        "                complete = (complete.bool() + (x_i[:, i] == self.eos)).float()\n",
        "            \n",
        "            return x\n",
        "\n",
        "    def loss(self, x):   \n",
        "        \"\"\"\n",
        "        Compute a scalar loss from a batch of sentences.\n",
        "        The loss is the negative log likelihood of the model estimated on a single batch:\n",
        "            - 1/batch_size * \\sum_{s} log P(x[s]|theta)\n",
        "\n",
        "        x: [batch_size, max_length] \n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Implement me!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syu-Z_-PkNwP"
      },
      "outputs": [],
      "source": [
        "seed_all()\n",
        "toy_lm = NeuralNGramLM(ngram_size=3, vocab_size=tokenizer.vocab_size(), embedding_dim=12, hidden_size=7)\n",
        "toy_lm.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRSWqNdmnfs7"
      },
      "outputs": [],
      "source": [
        "# I use 1 as EOS and 0 as BOS/PAD\n",
        "obs = torch.tensor(\n",
        "    [[5, 7, 6, 2, toy_lm.eos, toy_lm.pad],\n",
        "    [4, 5, 7, 4, 6, toy_lm.eos]]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EamB0ca0mC_t"
      },
      "source": [
        "Check that the forward pass is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RQIYa-plZ7K"
      },
      "outputs": [],
      "source": [
        "assert type(toy_lm(obs)) is td.Categorical, \"Did you change the return type?\"\n",
        "assert torch.allclose(torch.sum(toy_lm(obs).probs, -1), torch.ones_like(obs).float(), 1e-3), \"Your probabilities do not sum to 1\"\n",
        "assert toy_lm(obs).probs.shape == obs.shape + (tokenizer.vocab_size(),), \"The shape should be [2, 6, vocab_size]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Dnm6SW9VM5"
      },
      "source": [
        "We can estimate the loss using the 2 observations above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcJ_Dw5ykWmq"
      },
      "outputs": [],
      "source": [
        "toy_lm.loss(obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYWYSFLOYIq4"
      },
      "outputs": [],
      "source": [
        "assert type(toy_lm.loss(obs)) is torch.Tensor, \"Your loss should be a torch tensor\"\n",
        "assert toy_lm.loss(obs).requires_grad, \"Your loss should be differentiable\"\n",
        "assert toy_lm.loss(obs).shape == tuple(), \"Your loss should be a scalar tensor\"\n",
        "assert np.isclose(toy_lm.loss(obs).item(), 37, 1), \"Without training, with seed 42, and the obs we gave you, your loss should be close to 37. If this is not correct, check with your TA.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W_bVC0Dmn5d"
      },
      "source": [
        "<details>\n",
        "    <summary><b>Solution for forward method </b> </summary>\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "    \"\"\"\n",
        "    Parameterise the conditional distributions over X[i] given history x[:i] for i=1...I.\n",
        "\n",
        "    This procedure takes care that the ith output distribution conditions only on the n-1 observations before x[i].\n",
        "    It also takes care of padding to the left with BOS symbols.\n",
        "\n",
        "    x: [batch_size, max_length]\n",
        "\n",
        "    Return: a batch of V-dimensional Categorical distributions, one per step of the sequence.\n",
        "    \"\"\"\n",
        "    # The inputs to the FFNN are the ngram_size-1 previous words:\n",
        "\n",
        "    # Create a sequence of BOS symbols to be prepended to x.\n",
        "    # [batch_size, ngram_size - 1]\n",
        "    bos = torch.full((x.shape[0], self.ngram_size - 1), self.bos, device=x.device)\n",
        "    # [batch_size, max_length + ngram_size - 1]\n",
        "    _x = torch.cat([bos, x], 1)\n",
        "\n",
        "    # For each output step, we will have ngram_size - 1 inputs, so we collect those from x\n",
        "    # [batch_size, max_length, ngram_size - 1]\n",
        "    inputs = torch.cat([_x.unsqueeze(-1)[:,i:i+self.ngram_size-1].reshape(x.shape[0], 1, -1) for i in range(x.shape[1])], 1)\n",
        "\n",
        "    # Embed the input histories\n",
        "    # [batch_size, max_length, ngram_size - 1, D]\n",
        "    e = self.embed(inputs)\n",
        "    # [batch_size, max_length, (ngram_size - 1) * D]\n",
        "    e = e.reshape(x.shape + (-1,))\n",
        "\n",
        "    # Compute the V-dimensional scores (logits) \n",
        "    # [batch_size, max_length, V]\n",
        "    s = self.logits_predictor(e)\n",
        "\n",
        "    # For numerical stability, we prefer to parameterise the Categorical using logits, rather than probs.\n",
        "    # It would be equivalent (up to numerical precision) to use: Categorical(probs=F.softmax(s, -1))\n",
        "    return td.Categorical(logits=s)\n",
        "```    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCp-k3TIezCT"
      },
      "source": [
        "<details>\n",
        "    <summary> <b> Solution for loss method </b> </summary>\n",
        "\n",
        "```python\n",
        "def loss(self, x):   \n",
        "    \"\"\"\n",
        "    Compute a scalar loss from a batch of sentences.\n",
        "    The loss is the negative log likelihood of the model estimated on a single batch:\n",
        "        - 1/batch_size * \\sum_{s} log P(x[s]|theta)\n",
        "\n",
        "    x: [batch_size, max_length]    \n",
        "    \"\"\"\n",
        "    # [batch_size]\n",
        "    loss = - self.log_prob(x)    \n",
        "    # []\n",
        "    return loss.mean(0)        \n",
        "```        \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_tNbvLK9cb0"
      },
      "source": [
        "We can also obtain samples from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En3VysIstjW-"
      },
      "outputs": [],
      "source": [
        "toy_lm.sample(5, max_length=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BceKrvc9gBl"
      },
      "source": [
        "We can plot statistics of sampled strings, for example, length and word frequency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYNP65h0mTFu"
      },
      "outputs": [],
      "source": [
        "def compare_length_hist(obs, sample, title=\"\", pad_id=0):\n",
        "\n",
        "    bins = np.histogram_bin_edges(np.concatenate([np.array(( sample > 0)).sum(-1).flatten(), np.array(( obs > 0)).sum(-1).flatten()]), bins=10)\n",
        "    fig, axs = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(8, 4))\n",
        "\n",
        "    _ = axs[0].hist(np.array(( obs > 0)).sum(-1).flatten(), bins=bins)\n",
        "    _ = axs[1].hist(np.array(( sample > 0)).sum(-1).flatten(), bins=bins)\n",
        "\n",
        "    _ = axs[0].set_xlabel(\"Length of observed data\")\n",
        "    _ = axs[0].set_ylabel(\"Count\")\n",
        "    _ = axs[1].set_xlabel(\"Length of sampled data\")\n",
        "    _ = fig.suptitle(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K2BaK-m4bay"
      },
      "outputs": [],
      "source": [
        "sample = toy_lm.sample(100, max_length=20)\n",
        "compare_length_hist(obs, sample, \"Before training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODoUOx0g456b"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def compare_frequency(obs, sample, title=\"\", sharex=False, sharey=False):\n",
        "    fig, axs = plt.subplots(2, 2, sharex=sharex, sharey=sharey, figsize=(8, 8))\n",
        "    \n",
        "    \n",
        "    flat_obs = obs.flatten()\n",
        "    obs_counts = Counter(flat_obs[flat_obs > 0].numpy())\n",
        "    ocs = np.array([(r, w, n) for r, (w, n) in enumerate(obs_counts.most_common(), 1)])\n",
        "    \n",
        "    flat_samples = sample.flatten()\n",
        "    word_counts = Counter(flat_samples[flat_samples > 0].numpy())\n",
        "    wcs = np.array([(r, w, n) for r, (w, n) in enumerate(word_counts.most_common(), 1)])\n",
        "    \n",
        "    _ = axs[0, 0].plot(ocs[:,1], ocs[:,2]/ocs[:,2].sum(), 'x')    \n",
        "    _ = axs[1, 0].plot(wcs[:,1], wcs[:,2]/wcs[:,2].sum(), 'x')\n",
        "    \n",
        "    _ = axs[0, 0].set_ylabel(\"Frequency (observed)\")\n",
        "    _ = axs[1, 0].set_ylabel(\"Frequency (generated)\")\n",
        "    _ = axs[1, 0].set_xlabel(\"Word id\")\n",
        "    \n",
        "    _ = axs[0, 1].plot(ocs[:,0], ocs[:,2]/ocs[:,2].sum(), 'x')\n",
        "    _ = axs[1, 1].plot(wcs[:,0], wcs[:,2]/wcs[:,2].sum(), 'x')\n",
        "    _ = axs[1, 1].set_xlabel(\"Rank\")\n",
        "\n",
        "    _ = fig.suptitle(title)\n",
        "    _ = fig.tight_layout(h_pad=1, w_pad=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Iue1s_mTFu"
      },
      "outputs": [],
      "source": [
        "compare_frequency(obs, sample, \"Before training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI5-62hq9kQa"
      },
      "source": [
        "We can train the model using gradient-based optimisation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEfQ5Mltss4k"
      },
      "outputs": [],
      "source": [
        "optimiser = opt.Adam(toy_lm.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Az18f8rSX3"
      },
      "outputs": [],
      "source": [
        "with tqdm(range(1000)) as bar:\n",
        "    for _ in bar:\n",
        "        toy_lm.train()\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        loss = toy_lm.loss(obs)\n",
        "        bar.set_postfix({'loss': f\"{loss:.2f}\" } )\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT589emQ9piL"
      },
      "source": [
        "And then our samples should look less arbitrary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF1mRmgiklRV"
      },
      "outputs": [],
      "source": [
        "toy_lm.sample(10, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEOZNzjz9tNq"
      },
      "source": [
        "And statistics such as length and word frequency should be closer to training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAbkxJb-rKA_"
      },
      "outputs": [],
      "source": [
        "new_sample = toy_lm.sample(100, max_length=20)\n",
        "compare_length_hist(obs, new_sample, \"After training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JZ-zE0A40M_"
      },
      "outputs": [],
      "source": [
        "compare_frequency(obs, new_sample, \"After training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdxlZ_imZHCt"
      },
      "source": [
        "Now we will conduct an experiment with an actual corpus, we better use GPU support for that (on Google Colab you change the runtime to GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKFUw21bYCUm"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda:0')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "    print(\"You may continue with CPU, but when you get to the final experiment a CPU will be much too slow.\")\n",
        "my_device    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZVMkq0GZOqc"
      },
      "source": [
        "As we did in the PyTorch tutorial, we will create a `Dataset` object and a `DataLoader` for batching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhOlEqZbYTmU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPBI2HUXYkbz"
      },
      "outputs": [],
      "source": [
        "class Corpus(Dataset):\n",
        "\n",
        "    def __init__(self, corpus, tokenizer):\n",
        "        \"\"\"\n",
        "        In PyTorch we better always manipulate numerical codes, rather than text.\n",
        "        So, our Corpus object will contain a tokenizer that converts words to codes.\n",
        "\n",
        "        corpus: a list of sentences, each a string\n",
        "        tokenizer: a BPE tokenizer from sentencepiece\n",
        "        \"\"\"\n",
        "        self.corpus = list(corpus)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return corpus[idx] but BPE tokenized, converted to codes, and with the EOS symbol\"\"\"\n",
        "        return self.tokenizer.encode(self.corpus[idx], add_eos=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ02JHwwc6Gv"
      },
      "source": [
        "Our neural models are **a lot** more parameter efficient than our classic models, so we could use a larger vocabulary, but to keep the tutorial lightweight, we will still use a vocabulary of size 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z7xQXhHpSlX"
      },
      "outputs": [],
      "source": [
        "tokenizer = fit_vocabulary(training, vocab_size=1000)\n",
        "training_tok = Corpus(training, tokenizer)\n",
        "dev_tok = Corpus(dev, tokenizer)\n",
        "test_tok = Corpus(test, tokenizer)\n",
        "len(training_tok), len(dev_tok), len(test_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgwGUkWQZtuq"
      },
      "source": [
        "When we manipulate sequences of variable length, we need to \"pad\" them all to the same length. That's because to batch them using tensors they need to look like as if they did have the same length. \n",
        "\n",
        "We do that with a special symbol that will get ignored later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpR_1AkiadQo"
      },
      "outputs": [],
      "source": [
        "def pad_to_longest(sequences, pad_id=0):\n",
        "    \"\"\"\n",
        "    Take a list of coded sequences and returns a torch tensor where \n",
        "    every sentence has the same length (by means of using PAD tokens)\n",
        "    \"\"\"\n",
        "    longest = max(len(seq) for seq in sequences)\n",
        "    return torch.tensor([seq + [pad_id] * (longest - len(seq)) for seq in sequences])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cetno7rTaKPI"
      },
      "source": [
        "See what this does to the first few sentences in the batch (they should end with a few 0s, indicating they are shorter than the longest sentence in the batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7zeMNPgaEKZ"
      },
      "outputs": [],
      "source": [
        "pad_to_longest([training_tok[0], training_tok[1], training_tok[2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_duaGn_aOhx"
      },
      "source": [
        "Now that we can convert batches of sentences to codes and guarantee they have the same length, we can construct a data loader to create mini batches at random:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sHa1d5WpkdE"
      },
      "outputs": [],
      "source": [
        "batcher = DataLoader(training, batch_size=10, shuffle=True, collate_fn=pad_to_longest)\n",
        "len(batcher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FDyHpC7a2F9"
      },
      "source": [
        "We will need a batched PyTorch version of perplexity, to make sure you can run experiments with the correct code, we implement it here for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foxt9hp0n5J1"
      },
      "outputs": [],
      "source": [
        "def perplexity(model: NeuralNGramLM, dl, device):\n",
        "    \"\"\"\n",
        "    model: an instance of NeuralNGramLM\n",
        "    dl: a data loader for the heldout data\n",
        "    device: the PyTorch device where the model is stored\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_log_prob = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch in dl:\n",
        "            total_tokens += (batch > 0).float().sum()\n",
        "            total_log_prob = total_log_prob + model.log_prob(batch.to(device)).sum()\n",
        "    return torch.exp(-total_log_prob / total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7u12R0t3mAa"
      },
      "source": [
        "Here we have the training loop (already fully implemented for you). Do study it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQR5TGAqd-HC"
      },
      "outputs": [],
      "source": [
        "def train_neural_model(model, optimiser, training_corpus, dev_corpus, batch_size=200, num_epochs=10, device=torch.device('cuda:0')):\n",
        "    # we use the training data in random order for parameter estimation\n",
        "    batcher = DataLoader(training_corpus, batch_size=batch_size, shuffle=True, collate_fn=pad_to_longest)\n",
        "    # we use the dev data for evaluation during training (no need for randomisation here)\n",
        "    dev_batcher = DataLoader(dev_corpus, batch_size=batch_size, shuffle=False, collate_fn=pad_to_longest)\n",
        "\n",
        "    total_steps = num_epochs * len(batcher)\n",
        "    log = defaultdict(list)\n",
        "    ppl = perplexity(model, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    \n",
        "    step = 0\n",
        "\n",
        "    with tqdm(range(total_steps)) as bar:\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            for batch in batcher:\n",
        "                lm.train()  # this is how pytorch knows that we will be updating parameters\n",
        "                optimiser.zero_grad()  # this is needed in order to reset gradient information from previous iterations\n",
        "                \n",
        "                loss = lm.loss(batch.to(device))  # compute the loss for this batch\n",
        "                        \n",
        "                loss.backward()  # compute the gradient with respect to the parameters using backpropagation\n",
        "                optimiser.step()  # take a step towards minimising the loss\n",
        "\n",
        "                # udpate logging info\n",
        "                bar.set_postfix({'loss': f\"{loss.item():.2f}\", 'ppl': f\"{ppl:.2f}\"} )\n",
        "                bar.update()  \n",
        "                log['loss'].append(loss.item())\n",
        "\n",
        "                if step % 100 == 0:  # every so often, check performance on validation set\n",
        "                    ppl = perplexity(lm, dev_batcher, device=device).item()\n",
        "                    log['ppl'].append(ppl)\n",
        "                \n",
        "                step += 1\n",
        "                \n",
        "    ppl = perplexity(lm, dev_batcher, device=device).item()\n",
        "    log['ppl'].append(ppl)\n",
        "    return log            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwXhhAUChsKT"
      },
      "source": [
        "## <a name=\"neural-experiment\"/> Experiment\n",
        "\n",
        "Here we demonstrate how to train and evaluate a model. \n",
        "\n",
        "After that you will conduct an experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko-agWU5Giv"
      },
      "source": [
        "On GPU, this should take just about 2 minutes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EORnlrW3vAH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "seed_all() # reset random number generators before creating your model and training it\n",
        "\n",
        "# Create our LM\n",
        "lm = NeuralNGramLM(\n",
        "    ngram_size=3, \n",
        "    vocab_size=tokenizer.vocab_size(), \n",
        "    embedding_dim=32, \n",
        "    hidden_size=64, \n",
        "    pad_id=tokenizer.pad_id(),\n",
        "    bos_id=tokenizer.bos_id(),\n",
        "    eos_id=tokenizer.eos_id(),\n",
        ").to(my_device)\n",
        "\n",
        "# construct an Adam optimiser\n",
        "optimiser = opt.Adam(lm.parameters(), lr=5e-3)\n",
        "\n",
        "print(\"Model\")\n",
        "print(lm)\n",
        "# report number of parameters\n",
        "print(\"Model size:\", lm.num_parameters())\n",
        "\n",
        "log = train_neural_model(\n",
        "    lm, optimiser, training_tok, dev_tok, \n",
        "    batch_size=200, num_epochs=10, \n",
        "    device=my_device\n",
        ")\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "_ = axs[0].plot(np.arange(len(log['loss'])), log['loss'])\n",
        "_ = axs[0].set_xlabel('steps')\n",
        "_ = axs[0].set_ylabel('training loss')\n",
        "_ = axs[1].plot(np.arange(len(log['ppl'])), log['ppl'])\n",
        "_ = axs[1].set_xlabel('steps (in 100s)')\n",
        "_ = axs[1].set_ylabel('model ppl given dev')\n",
        "_ = fig.tight_layout(h_pad=2, w_pad=2)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n# Samples\\n\\n\")\n",
        "# lm.sample(10, 30) returns 10 samples (of at most 30 tokens each) in a torch tensor\n",
        "# we can use the tensor's method `.tolist()` in order to convert the tensor\n",
        "# to a standard python list, which our tokenizer knows how to convert to strings\n",
        "for i, x in enumerate(tokenizer.decode(lm.sample(10, 30).tolist(), out_type=str), 1):\n",
        "    print(f\"{i}\\t{x}\")\n",
        "\n",
        "test_batcher = DataLoader(test_tok, batch_size=100, shuffle=False, collate_fn=pad_to_longest)\n",
        "print(\"\\n\\n Model perplexity given test set\", perplexity(lm, test_batcher, device=my_device).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjGAF2hQegly"
      },
      "source": [
        "<a name=\"report\">  **Graded exercise - Neural NGram LM**\n",
        "\n",
        "**For this exercise, make sure to use a GPU on Colab.**\n",
        "\n",
        "Train NeuralNGram LMs using the Brown corpus and a BPE vocabulary of 1000 tokens.\n",
        "\n",
        "* Train 2-gram LMs, 3-gram LMs, 4-gram LMs and 5-gram LMs\n",
        "* For each model, train it with \n",
        "    * 32 dimensions for embeddings and 64 dimensions for hidden size\n",
        "    * 64 dimensions for embeddings and 128 dimensions for hidden size    \n",
        "* For each model, plot the training loss and perplexity using the dev set throughout training. \n",
        "* For trained models, report a table with embedding size, hidden size, ngram-size, model size (in number of parameters), model perplexity using the dev set, and model perplexity using the test set\n",
        "* As always, write a few discussion points. Here are some ideas: comment on model growth (in number of parameters) as you increase ngram-size, how it compares to the growth of a classic model using tabular cpds, comment on the effect of incresing the number of embedding and/or hidden units.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-iWctvV50An"
      },
      "source": [
        "<a name=\"stats\"> **Graded exercises - Analysis**\n",
        "\n",
        "Using the best model you have (measured in perplexity given dev set):\n",
        "\n",
        "* Draw samples from the model (use the same number of sentences as in the dev set)\n",
        "* Compare the length distribution of generated data with the length distribution of the dev set (use BPE-tokenization for that). Make remarks.\n",
        "* BPE-detokenize your samples and re-tokenize it using python `.split()`, investigate what percentage of generated words did not exist in the **training data**.\n",
        "* Manually inspect a subset of these new tokens, study the 30 most frequent such tokens. Comment on what you observe and relate your observations to technical aspects of the model. How many of these tokens are words that do not exist in English?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2022/T5_teacher",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}